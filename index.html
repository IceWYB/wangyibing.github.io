<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META name="description" content="Yibing Wang's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK href="files/doc.css" 
    rel="stylesheet" type="text/css"> 
  <TITLE>Yibing Wang</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Yibing Wang &nbsp;</H1></DIV>
        <H3>Ph.D. candidate</H3>
        <BR>School of Electronic, Electric and Communication System
        <BR>University of Chinese Academy of Science
        <BR>Beijing, China.
        <BR>
        <BR> Email:  
        <A href="mailto:wangyibing18@mails.ucas.ac.cn"> wangyibing18@mails.ucas.ac.cn</A>; 
        <BR> Github: 
        <A href="https://github.com/IceWYB">https://github.com/IceWYB</A>;
        <BR> Google scholar:
        <A href="https://scholar.google.co.uk/citations?user=zNk0c_0AAAAJ&hl=zh-CN&oi=sra">https://scholar.google.co.uk/citations?user=zNk0c_0AAAAJ&hl=zh-CN&oi=sra</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/personal_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>Biography</H2>
  <P> I am a Ph.D. candidate in the School of Electronic, Electric and Communication System, University of Chinese Academy of Science</A>, 
    advised by <A href="https://scholar.google.com/citations?user=XqdpqNcAAAAJ">Prof. Jianbin Jiao</A>.
    I got a B.E. degree in the University of Chinese Academy of Science in June 2022.
  </P>

  <P>My research interests include computer vision and deep learning, specifically for multimodal learning.</P>

  <H2>Academic Services</H2>
    <P> Journal Reviewer: IJCV, ICML, CVPR, etc.</P>

  <!-- <H2>News</H2>
    <P> [2024.08] <b>Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input </b> is available on <A href="https://arxiv.org/pdf/2408.15542"> Arkiv</A>. </P>
     -->
  <H2>Publications</H2>
    <table class="pub_table">
    <!-- <tbody> -->
      <tr>
        <td class="pub_td1"><img src="files/kangaroo.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast; Jiajun Liu, &ast; <u>Yibing Wang</u>, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu
          <br><b>Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input</b>
          <br>
          [<a href="https://arxiv.org/pdf/2408.15542">Paper</a>]
          [<a href="https://kangaroogroup.github.io/Kangaroo.github.io">Project</a>]
          [<a href="https://github.com/KangarooGroup/Kangaroo">Code</a>]
          [<a href="https://huggingface.co/KangarooGroup/kangaroo">Model</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/x-omni.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast; Zigang Geng, &ast; <u>Yibing Wang</u>, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang and Jie Jiang
          <br><b>X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</b>
          <br>
          [<a href="https://arxiv.org/pdf/2507.22058">Paper</a>]
          [<a href="https://github.com/X-Omni-Team/X-Omni">Code</a>]
          [<a href="https://x-omni-team.github.io/">Project</a>]
          [<a href="https://huggingface.co/collections/X-Omni/x-omni-models-6888aadcc54baad7997d7982">Model</a>]
          [<a href="https://huggingface.co/collections/X-Omni/x-omni-spaces-6888c64f38446f1efc402de7">Space</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/odyssey.png" class="papericon"></td>
        <td 
          class="pub_td2">Gong, Kaixiong and Feng, Kaituo and Li, Bohao and <u>Yibing Wang</u> and Cheng, Mofan and Yang, Shijia and Han, Jiaming and Wang, Benyou and Bai, Yutong and Yang, Zhuoran
          <br><b>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</b>
          <br>
          [<a href="https://arxiv.org/pdf/2412.02611">Paper</a>]
          [<a href="https://av-odyssey.github.io">Project</a>]
          [<a href="https://github.com/AV-Odyssey/AV-Odyssey">Code</a>]
          [<a href="https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench">Dataset</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/video-r1.png" class="papericon"></td>
        <td 
          class="pub_td2">Feng, Kaituo and Gong, Kaixiong and Li, Bohao and Guo, Zonghao and <u>Wang, Yibing</u> and Peng, Tianshuo and Wang, Benyou and Yue, Xiangyu
          <br><b>X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</b>
          <br>
          [<a href="https://arxiv.org/pdf/2503.21776">Paper</a>]
          [<a href="https://github.com/tulerfeng/Video-R1">Code</a>]
          [<a href="https://huggingface.co/Video-R1/Video-R1-7B">Model</a>]
          [<a href="https://huggingface.co/datasets/Video-R1/Video-R1-data">Data</a>]
          <br>
        </td>
      </tr>

</BODY>
</HTML>
